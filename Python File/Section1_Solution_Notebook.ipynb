{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Hup7XFgnzo"
   },
   "source": [
    "# **UK HOUSE PRICES REDICTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwVkEQ9Kgbpq"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9Za86VLLgQwb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from typing import Union\n",
    "import joblib\n",
    "import time\n",
    "#!pip install category_encoders\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeryznQxhETU"
   },
   "source": [
    "**Defining Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yfW5qGBhlSR"
   },
   "source": [
    "Creating all functions which will be used in our machine learning workflow to\n",
    "train our model, perform exploratory data analysis, save our model, and perform\n",
    "visualizations on the bean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_01lXB7Whxm8"
   },
   "outputs": [],
   "source": [
    "def build_regressor_model(regressor,\n",
    "                          x_train: pd.DataFrame,\n",
    "                          y_train: pd.DataFrame,\n",
    "                          x_test: pd.DataFrame,\n",
    "                          y_test: pd.DataFrame,\n",
    "                          kfold: int = 10):\n",
    "    # Model Training\n",
    "    model = regressor.fit(x_train, y_train)\n",
    "\n",
    "    # Model Prediction\n",
    "    y_pred = model.predict(x_train) # Training Predictions: Check OverFitting\n",
    "    y_pred1 = model.predict(x_test) # Test Predictions: Check Model Predictive Capacity\n",
    "\n",
    "    # Model Evaluation\n",
    "    # Training Evaluation: Check OverFitting\n",
    "    training_rsquared = r2_score(y_train, y_pred)\n",
    "    training_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "\n",
    "    # Test Evaluations: Check Model Predictive Capacity\n",
    "    test_rsquared = r2_score(y_test, y_pred1)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
    "\n",
    "    # Validation of Predictions\n",
    "    cross_val = cross_val_score(model, x_train, y_train, cv = kfold)\n",
    "    cross_validation = cross_validate(model,\n",
    "                                      x_train,\n",
    "                                      y_train,\n",
    "                                      cv = kfold,\n",
    "                                      return_estimator = True,\n",
    "                                      return_train_score = True)\n",
    "    score_mean = round((cross_val.mean() * 100), 2)\n",
    "    score_std_dev = round((cross_val.std() * 100), 2)\n",
    "\n",
    "    # Visualization\n",
    "    # Visualising the actual testing data and predicted values\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.grid(True)\n",
    "    plt.scatter(y_test, y_pred1, color='blue', alpha=0.5, label = {\"Test RMSE\": test_rmse})\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"--c\", label = {\"Test R-Squared\": test_rsquared})\n",
    "    plt.title(f'Analyzing the Actual values against the Predicted Values - {regressor.__class__.__name__}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Model\": model,\n",
    "        \"Predictions\": {\n",
    "            \"Actual Training Y\": y_train,\n",
    "            \"Actual Test Y\": y_test,\n",
    "            \"Predicted Training Y\": y_pred,\n",
    "            \"Predicted Test Y\": y_pred1\n",
    "            },\n",
    "        \"Training Evaluation\": {\n",
    "            \"Training R2\": training_rsquared,\n",
    "            \"Training RMSE\": training_rmse\n",
    "            },\n",
    "        \"Test Evaluation\": {\n",
    "            \"Test R2\": test_rsquared,\n",
    "            \"Test RMSE\": test_rmse\n",
    "            },\n",
    "        \"Cross Validation\": {\n",
    "            \"Cross Validation Mean\": score_mean,\n",
    "            \"Cross Validation Standard Deviation\": score_std_dev,\n",
    "            \"Validation Models\": cross_validation\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkvCZtMiiheh"
   },
   "outputs": [],
   "source": [
    "def build_multiple_regressors(regressors: Union[list or tuple],\n",
    "                              x_train: pd.DataFrame,\n",
    "                              y_train: pd.DataFrame,\n",
    "                              x_test: pd.DataFrame,\n",
    "                              y_test: pd.DataFrame,\n",
    "                              kfold: int = 10):\n",
    "    multiple_regressor_models = {} # General store for all metrics from each algorithm\n",
    "    store_algorithm_metrics = [] # Store all metrics gotten from the algorithm at each iteration in the loop below\n",
    "    dataframe = pd.DataFrame(columns = [\"Algorithm\",\n",
    "                                        \"Fit time\",\n",
    "                                        \"Score time\",\n",
    "                                        \"Test score\",\n",
    "                                        \"Train score\"]) # Store cross validation metrics\n",
    "\n",
    "    # Creating a dataframe for all classifiers\n",
    "    # ---> Loop through each classifier ain classifiers and do the following\n",
    "    for algorithms in regressors:\n",
    "        store_cross_val_models = {}\n",
    "\n",
    "        # Call the function build_classifier_model to get classifier metrics\n",
    "        print(f\"Building regressor model and metrics for {algorithms.__class__.__name__} model.\")\n",
    "        multiple_regressor_models[f\"{algorithms.__class__.__name__}\"] = build_regressor_model(regressor = algorithms,\n",
    "                                                                                              x_train = x_train,\n",
    "                                                                                              y_train = y_train,\n",
    "                                                                                              x_test = x_test,\n",
    "                                                                                              y_test = y_test,\n",
    "                                                                                              kfold = kfold)\n",
    "\n",
    "        # Collecting individual metric to build algorithm dataframe\n",
    "        training_r2 = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Training Evaluation\"][\"Training R2\"]\n",
    "        training_rmse = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Training Evaluation\"][\"Training RMSE\"]\n",
    "        test_r2 = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Test Evaluation\"][\"Test R2\"]\n",
    "        test_rmse = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Test Evaluation\"][\"Test RMSE\"]\n",
    "        cross_val_mean = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Cross Validation Mean\"]\n",
    "        cross_val_std = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Cross Validation Standard Deviation\"]\n",
    "\n",
    "        # Collecting indiviual metric to build cross validation dataframe\n",
    "        cross_val_fit_time = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Validation Models\"][\"fit_time\"]\n",
    "        cross_val_score_time = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Validation Models\"][\"score_time\"]\n",
    "        cross_val_test_score = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Validation Models\"][\"test_score\"]\n",
    "        cross_val_train_score = multiple_regressor_models[f\"{algorithms.__class__.__name__}\"][\"Cross Validation\"][\"Validation Models\"][\"train_score\"]\n",
    "\n",
    "        # Storing all individual algorithm metrics from each iteration\n",
    "        store_algorithm_metrics.append([algorithms.__class__.__name__,\n",
    "                                        training_r2,\n",
    "                                        training_rmse,\n",
    "                                        test_r2,\n",
    "                                        test_rmse,\n",
    "                                        cross_val_mean,\n",
    "                                        cross_val_std])\n",
    "        # Storing all individual cross validation metrics from each iteration\n",
    "        store_cross_val_models[\"Algorithm\"] = algorithms.__class__.__name__\n",
    "        store_cross_val_models[\"Fit time\"] = cross_val_fit_time\n",
    "        store_cross_val_models[\"Score time\"] = cross_val_score_time\n",
    "        store_cross_val_models[\"Test score\"] = cross_val_test_score\n",
    "        store_cross_val_models[\"Train score\"] = cross_val_train_score\n",
    "        # Creating dataframe for cross validation metric\n",
    "        data_frame = pd.DataFrame(store_cross_val_models)\n",
    "        dataframe = pd.concat([dataframe, data_frame])\n",
    "        print(\"Model building completed.\\n\")\n",
    "\n",
    "    # Creating dataframe for algorithm metric\n",
    "    df = pd.DataFrame(store_algorithm_metrics, columns = [\"Algorithm\",\n",
    "                                                          \"Training R2\",\n",
    "                                                          \"Training RMSE\",\n",
    "                                                          \"Test R2\",\n",
    "                                                          \"Test RMSE\",\n",
    "                                                          \"CV Mean\",\n",
    "                                                          \"CV Standard Deviation\"])\n",
    "    # Save datasets in folder for analysis\n",
    "    save_dataframe(dataset = dataframe, name = \"Cross_Validation_Evaluation\")\n",
    "    save_dataframe(dataset = df, name = \"Algorithm_Evaluation\")\n",
    "\n",
    "    return (df, dataframe, multiple_regressor_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXKthriKg9Co"
   },
   "outputs": [],
   "source": [
    "def eda(dataset: pd.DataFrame, graphs: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Dataset to perform EDA.\n",
    "    graphs : bool, optional\n",
    "        Choose to display exploratory data analysis visuals. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing different evaluation metrics for exploring the\n",
    "        columns and understanding how values in the dataset are distributed.\n",
    "\n",
    "    \"\"\"\n",
    "    data_unique = {}\n",
    "    data_category_count = {}\n",
    "    dataset.info()\n",
    "    data_head = dataset.head()\n",
    "    data_tail = dataset.tail()\n",
    "    data_mode = dataset.mode().iloc[0]\n",
    "    data_descriptive_stats = dataset.describe()\n",
    "    data_more_descriptive_stats = dataset.describe(include = \"all\",\n",
    "                                                   datetime_is_numeric=True)\n",
    "    data_correlation_matrix = dataset.corr(numeric_only = True)\n",
    "    data_distinct_count = dataset.nunique()\n",
    "    data_count_duplicates = dataset.duplicated().sum()\n",
    "    data_count_null = dataset.isnull().sum()\n",
    "    data_total_null = dataset.isnull().sum().sum()\n",
    "    for each_column in dataset.columns: # Loop through each column and get the unique values\n",
    "        data_unique[each_column] = dataset[each_column].unique()\n",
    "    for each_column in dataset.select_dtypes(object).columns:\n",
    "        # Loop through the categorical columns and count how many values are in each category\n",
    "        data_category_count[each_column] = dataset[each_column].value_counts()\n",
    "\n",
    "    if graphs == True:\n",
    "        # Visuals\n",
    "        dataset.hist(figsize = (25, 20), bins = 10)\n",
    "        plt.figure(figsize = (15, 10))\n",
    "        sns.heatmap(data_correlation_matrix, annot = True, cmap = 'coolwarm')\n",
    "        plt.show()\n",
    "        plt.figure(figsize = (50, 30))\n",
    "        sns.pairplot(dataset) # Graph of correlation across each numerical feature\n",
    "        plt.show()\n",
    "\n",
    "    result = {\"data_head\": data_head,\n",
    "              \"data_tail\": data_tail,\n",
    "              \"data_mode\": data_mode,\n",
    "              \"data_descriptive_stats\": data_descriptive_stats,\n",
    "              \"data_more_descriptive_stats\": data_more_descriptive_stats,\n",
    "              \"data_correlation_matrix\": data_correlation_matrix,\n",
    "              \"data_distinct_count\": data_distinct_count,\n",
    "              \"data_count_duplicates\": data_count_duplicates,\n",
    "              \"data_count_null\": data_count_null,\n",
    "              \"data_total_null\": data_total_null,\n",
    "              \"data_unique\": data_unique,\n",
    "              \"data_category_count\": data_category_count,\n",
    "              }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa-u2f2fiHFZ"
   },
   "outputs": [],
   "source": [
    "def save_dataframe(dataset: pd.DataFrame, name: str):\n",
    "    \"\"\"\n",
    "    Save the data to the generated_data folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Dataset containing the information we want to save. For this project,\n",
    "        it could be a dataframe of algorithm metrics or cross validation metrics.\n",
    "    name: str\n",
    "        A string indicating the name of the dataset and how it should be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_name = name\n",
    "        date = time.strftime(\"%Y-%m-%d\")\n",
    "        dataset.to_csv(f\"../../Datasets/generated_data/section1/{data_name}_{date}.csv\", index = False)\n",
    "        print(\"\\nSuccessfully saved file to the specified folder ---> generated_data folder.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nFailed to save file to the specified folder ---> generated_data folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsMIubvGh968"
   },
   "outputs": [],
   "source": [
    "def save_model_from_cross_validation(models_info: dict, algorithm: str, index: None):\n",
    "    model_to_save = models_info[algorithm][\"Cross Validation\"][\"Validation Models\"][\"estimator\"][index]\n",
    "\n",
    "    # Using Joblib to save the model in our folder\n",
    "    joblib.dump(model_to_save, f\"models/{algorithm}_Model_{index}.pkl\")\n",
    "    print(f\"\\nThis model is gotten from cross validating with the {algorithm} algorithm at iteration {index + 1}.\")\n",
    "    return models_info[algorithm][\"Cross Validation\"][\"Validation Models\"][\"estimator\"][index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ7M4d31i45R"
   },
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "You are required to carry out the following tasks:\n",
    "\n",
    "a. Identify and address any issues in the dataset. Then conduct exploratory data\n",
    "analysis on the dataset (10 marks)\n",
    "\n",
    "b. Analyse the relationships between price and the other features. What are your\n",
    "conclusions? (10 marks)\n",
    "\n",
    "c. Analyse and visualise the impact of Brexit on house prices. What are your\n",
    "conclusions? (10 marks)\n",
    "\n",
    "d. Build a predictive model to estimate house prices. You need to show how to:\n",
    "\n",
    "        • Use appropriate methods to select relevant features.\n",
    "        • Split the dataset into training and testing sets.\n",
    "        • Train at least 3 different prediction models.\n",
    "        • Evaluate the models with the performance, and report in terms of R2 and\n",
    "        RMSE.\n",
    "        • Visualise the actual testing data and predicted values.\n",
    "        • State your conclusions on the models.\n",
    "        • Save your best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYvQBXzhkCfs"
   },
   "source": [
    "BEFORE ANY SOLUTION, WE WOULD NEED TO IMPORT OUR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GO_iVB5Fj8yn",
    "outputId": "d6855a64-6cef-4e85-a76a-f834054c9a03"
   },
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "dataset = pd.read_csv(\"../../Datasets/Coursework Datasets/UK_Housing_Data.csv\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLtLH_d8jAoY"
   },
   "source": [
    "**SOLUTION A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyYQ5Fy7johm"
   },
   "source": [
    "Perform exploratory data analysis to gain insight on the dataset and possible issues that need to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CTCm7eYwj2wS",
    "outputId": "20d14791-40e6-446d-b4d0-9340b5842d46"
   },
   "outputs": [],
   "source": [
    "# Exploratory data analysis\n",
    "initial_eda = eda(dataset, graphs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyPVu1zrmoJ6"
   },
   "source": [
    "**Data cleaning and Transformation**\n",
    "    # --- Issues:\n",
    "1) We have 1828855 missing values initially in our data. After doing some initial data preprocessing, we are left with 23741 missing values in our data to handle.\n",
    "  * One possible fix is using the SimpleImputer from sklearn.impute or KNNImputer from sklearn.impute.\n",
    "  * Another fix could be to drop missing rows.\n",
    "\n",
    "2) TID (Transaction Identifier) has curly braces around each value.\n",
    "  * Remove them using .replace()\n",
    "\n",
    "3) We have some irrelevant columns for the house prices analysis. Some of the\n",
    "irrelevant columns we have include:\n",
    "\n",
    "    a) Unnamed: 0\n",
    "    b) TID\n",
    "    c) SAON\n",
    "    d) PAON\n",
    "    e) Record Status\n",
    "\n",
    "The features SAON and PAON are considered irrelevant as the house number doesn't help us in predicting the price of a house, neither does the flat number. The SAON feature also has over a million rows missing in the dataset.\n",
    "\n",
    "The feature TID (Transaction ID) is irrelevant for this analysis given it is just an identifier for the transactions.\n",
    "\n",
    "The feature Record Status has one unique value (A), therefore, this won't be useful for gaining any insights given the absolute value is always (A) meaning variance is zero.\n",
    "  * Drop irrelevant columns using the .drop() pandas command.\n",
    "\n",
    "4) TDate (Transaction Date) is given of type object.\n",
    "  * This is false and needs to be replaced with type datetime64.\n",
    "    \n",
    "5) Drop the LOCALITY column. More than half of the data is missing in that column.\n",
    "    \n",
    "6) Remove duplicate columns created after the above preprocessing steps. 2409 duplicated values were created that need to be dropped.\n",
    "  * Using the .drop_duplicates command solves this problem.\n",
    "\n",
    "7) The TDate column needs to be processed to extract date features such as year, month, and day, for our model creation.\n",
    "    \n",
    "8) Drop categorical columns that aren't correlated with Price to avoid create a complex model and introducing noise into our model, further reducing our models predictive power.\n",
    "\n",
    "9) Handling the categorical features for prediction. This process has a huge influence on the predictive power of our model, hence it is very crucial.\n",
    "  * One possible solution will be to use LabelEncoder from sci-kit learn or TargetEncoder from category_encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMnWQjjgofy1"
   },
   "outputs": [],
   "source": [
    "# TID has curly braces --- Remove Them\n",
    "dataset[\"TID\"] = dataset[\"TID\"].replace({\"{\": \"\",\n",
    "                                          \"}\": \"\"})\n",
    "# Drop irrelevant columns\n",
    "dataset = dataset.drop([\"TID\",\n",
    "                        \"Unnamed: 0\",\n",
    "                        \"SAON\",\n",
    "                        \"PAON\",\n",
    "                        \"RecordStatus\"], axis = 1)\n",
    "\n",
    "# Drop locality columns due to more than half of the data in the column missing\n",
    "dataset = dataset.drop([\"Locality\"], axis = 1)\n",
    "\n",
    "# Storing clean dataset for visualization\n",
    "data = dataset\n",
    "\n",
    "# Transaction date to datetime64\n",
    "dataset[\"TDate\"] = pd.to_datetime(dataset[\"TDate\"])\n",
    "\n",
    "# Drop duplicates\n",
    "dataset = dataset.drop_duplicates()\n",
    "\n",
    "# Fix missing values\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a29INXOvoykv"
   },
   "source": [
    "Next, we create some hierarchy in our data given we have time sensitive information using the TDate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rQhslcypFNL",
    "outputId": "386934cb-069b-449f-883a-068fe7672449"
   },
   "outputs": [],
   "source": [
    "# Sort Dates column\n",
    "dataset = dataset.sort_values(\"TDate\", ascending = True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkKfyQDbpNZP"
   },
   "source": [
    "We then perform some feature engineering to extract time related features from the TDate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yB5ZpQVSpZyS",
    "outputId": "2494cae2-f9d4-4d59-c2dc-230ac802b99f"
   },
   "outputs": [],
   "source": [
    "# Extract time features\n",
    "dataset[\"Year\"] = dataset[\"TDate\"].dt.year\n",
    "dataset[\"Month\"] = dataset[\"TDate\"].dt.month\n",
    "dataset[\"Day\"] = dataset[\"TDate\"].dt.day\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFS1EON4rD5R"
   },
   "source": [
    "**Further Data Preparation and Segregation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCbK8pj6pqLC"
   },
   "source": [
    "Next, we perform some further data preparation and segregation in this section. These includes:\n",
    "\n",
    "    * Selecting dependent and independent variables\n",
    "    * Encoding categorical features\n",
    "    * Feature selection\n",
    "    * Splitting the data into training and testing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnNyZe--rRRg"
   },
   "source": [
    "Selecting dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFS2lsSCrNn8",
    "outputId": "ebb5aeff-d178-4f11-bdca-1121c7a1bc8c"
   },
   "outputs": [],
   "source": [
    "X = dataset.drop([\"Price\"], axis=1)\n",
    "y = dataset.Price\n",
    "\n",
    "print(X)\n",
    "print(\"\\n\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYWVKFbnrVgu"
   },
   "source": [
    "Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_vQJ10LrYqd",
    "outputId": "b24ff8b0-1dfb-4acb-f789-cb5d2097be2a"
   },
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder(smoothing = 50, min_samples_leaf = 20)\n",
    "X = encoder.fit_transform(X, y)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBg8oZE6s7jR"
   },
   "source": [
    "**SOLUTION B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW3zGOhGtCtU"
   },
   "source": [
    "Finding the correlation between the encoded features and price. Checks for linear relationship between the features and price. For checking correlation, we use Pearson Product Moments.\n",
    "\n",
    "We will find the correlation between these features before feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqbQjkZMtmV8",
    "outputId": "7de441c2-3928-412d-a909-e6bfc7ed40ed"
   },
   "outputs": [],
   "source": [
    "correlation_between_features = pd.concat([X, y], axis = 1).corr()\n",
    "\n",
    "print(correlation_between_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3C8DOYnxKiv"
   },
   "source": [
    "**CONCLUSIONS**\n",
    "\n",
    "From our analysis of the correlation matrix, insights show a positive linear relationship between postcode and price, as well as street and price. All other features don't have either no correlation with price or a relatively low correlation with price.\n",
    "\n",
    "Including these uncorrelated columns into our model can introduce noise and lead to a complex model. The features postcode and street can be used to implement linear models with high accuracy given the approximate 0.97 and 0.67 positive relationship with price respectively.\n",
    "\n",
    "These two features can succesfully capture the linear variations in price measured by r-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3pHD5O8A1Uh"
   },
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWZ-bg4-BDM_"
   },
   "source": [
    "Conducting EDA to get a view of our dataset after data cleaning and transformation to validate steps and approach before model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gemo65fBPel",
    "outputId": "46c314fa-18d2-4c70-a242-958fa83dffca"
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "data_eda = eda(correlation_between_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UiZQMf6yjI7"
   },
   "source": [
    "**SOLUTION C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzYO7kIQypDF"
   },
   "source": [
    "Analysing the impact of Brexit on UK house prices involves 4 approaches. Hence, we break our analysis into 4 sections, then a final conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr0v-ARizEF9"
   },
   "source": [
    "    --- SECTION 1: Analyzing yearly average house prices during UK Brexit transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XlAd7zKzQop",
    "outputId": "d230e295-79da-4e44-d0fc-a20214ce51a5"
   },
   "outputs": [],
   "source": [
    "year_data = dataset[[\"Price\", \"TDate\", \"Year\", \"Month\", \"Day\"]]\n",
    "year_data = year_data.drop([\"TDate\", \"Month\", \"Day\"], axis = 1)\n",
    "year_data = year_data.groupby(\"Year\").mean(numeric_only = True).reset_index()\n",
    "\n",
    "segment_ranges = [(2010, 2016), (2016, 2020), (2020, 2023)]\n",
    "colors = ['blue', 'red', 'green']\n",
    "info = [\"Before Referendum\", \"During Negotiations for Brexit\", \"Brexit Begins\"]\n",
    "\n",
    "print(year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "iIorlAv-zXQH",
    "outputId": "f06ffd6f-3a9e-44e7-f67b-11f0b7f89634"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize = (35, 10))\n",
    "plt.title(\"Analyzing Yearly Average Prices in UK\")\n",
    "plt.plot(year_data[\"Year\"], year_data[\"Price\"], \"--g\", marker = 'o', markersize = 10, alpha = 0.5)\n",
    "plt.axvline(x = 2016, color='cyan', linestyle='--', linewidth=2, alpha = 0.9, label = \"Referendum for Brexit\")\n",
    "plt.axvline(x = 2020, color='cyan', linestyle='--', linewidth=2, alpha = 0.9, label = \"UK leaves European Union(EU)\")\n",
    "for i, (start, end) in enumerate(segment_ranges):\n",
    "    segment_data = year_data[(year_data[\"Year\"] >= start) & (year_data[\"Year\"] <= end)]\n",
    "    plt.plot(segment_data[\"Year\"], segment_data[\"Price\"], marker='o', markersize=10, color=colors[i], label = info[i])\n",
    "plt.xlabel(\"Year\", labelpad = 20)\n",
    "plt.ylabel(\"Average Price\", labelpad = 20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adbB50GTz5AX"
   },
   "source": [
    "    --- SECTION 2: Analyzing monthly average house prices during UK Brexit transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bkj-Xm0E0DA_",
    "outputId": "95c855a9-1780-4ca7-d673-d008021f474d"
   },
   "outputs": [],
   "source": [
    "month_data = dataset\n",
    "month_data[\"Year-Month\"] = month_data[\"Year\"].astype(str) + \"-\" + month_data[\"Month\"].astype(str)\n",
    "month_data = month_data.drop([\"TDate\", \"Month\", \"Year\", \"Day\"], axis = 1)\n",
    "month_data = month_data.groupby(\"Year-Month\").mean(numeric_only = True).reset_index()\n",
    "\n",
    "segment_ranges1 = [(\"2010-1\", \"2016-1\"), (\"2016-1\", \"2020-2\"), (\"2020-2\", \"2023-4\")]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "print(month_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "97lnSO110FVL",
    "outputId": "7990c51d-35fb-4fe4-804a-d0e52ab03241"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize = (45, 10))\n",
    "plt.title(\"Analyzing Monthly Average Prices in UK\")\n",
    "plt.plot(month_data[\"Year-Month\"], month_data[\"Price\"], \"--g\", marker = 'o', markersize = 5, alpha = 0.5)\n",
    "plt.axvline(x = \"2016-1\", color='cyan', linestyle='--', linewidth=2, alpha = 0.9, label = \"Referendum for Brexit\")\n",
    "plt.axvline(x = \"2020-2\", color='cyan', linestyle='--', linewidth=2, alpha = 0.9, label = \"UK leaves European Union(EU)\")\n",
    "for i, (start, end) in enumerate(segment_ranges1):\n",
    "    segment_data1 = month_data[(month_data[\"Year-Month\"] >= start) & (month_data[\"Year-Month\"] <= end)]\n",
    "    plt.plot(segment_data1[\"Year-Month\"], segment_data1[\"Price\"], marker='o', markersize=10, color=colors[i], label = info[i])\n",
    "plt.xticks(rotation = 90, ha ='right')\n",
    "plt.xlabel(\"Months\", labelpad = 20)\n",
    "plt.ylabel(\"Average Price\", labelpad = 20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmR7pBqb0TLc"
   },
   "source": [
    "    --- SECTION 3: Analyzing maximum house prices before referendum for brexit, during the transition and after brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftpiy0sv0aJF",
    "outputId": "c35d2334-3f93-48cb-da11-1438798011e9"
   },
   "outputs": [],
   "source": [
    "before_referendum_brexit = round(year_data[year_data[\"Year\"] <= 2016][\"Price\"].max(), 2)\n",
    "during_negotiations_brexit = round(year_data[(year_data[\"Year\"] > 2016) & (year_data[\"Year\"] <= 2020)][\"Price\"].max())\n",
    "after_brexit = round(year_data[year_data[\"Year\"] > 2020][\"Price\"].max())\n",
    "y_max = [before_referendum_brexit, during_negotiations_brexit, after_brexit]\n",
    "X_max = [\"Before Referendum\", \"During Negotiations for Brexit\", \"Brexit Begins\"]\n",
    "\n",
    "print(\"Before Referendum -\", before_referendum_brexit)\n",
    "print(\"During Negotiations for Brexit -\", during_negotiations_brexit)\n",
    "print(\"Brexit Begins -\", after_brexit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "kU-pEVxk0e3u",
    "outputId": "64e5bada-0dc5-4fb5-b935-4f9365961804"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.title(\"Analyzing Maximum Yearly Average House Prices in UK during Brexit Transition\")\n",
    "container = plt.bar(X_max, y_max, width = 0.5, color = colors, alpha = 0.5)\n",
    "plt.bar_label(container, labels = y_max, padding = 10)\n",
    "plt.xlabel(\"Brexit\", labelpad = 20)\n",
    "plt.ylabel(\"Maximum House Price\", labelpad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MnAhn6_0Ppf"
   },
   "source": [
    "    --- SECTION 4: Analyzing house prices before referendum for brexit, during the transition and after brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAn3mtVP0mKs",
    "outputId": "2006ab99-f483-4cab-d924-fa742de2c7ac"
   },
   "outputs": [],
   "source": [
    "before_referendum_brexit_min = round(year_data[year_data[\"Year\"] <= 2016][\"Price\"].min(), 2)\n",
    "during_negotiations_brexit_min = round(year_data[(year_data[\"Year\"] > 2016) & (year_data[\"Year\"] <= 2020)][\"Price\"].min())\n",
    "after_brexit_min = round(year_data[year_data[\"Year\"] > 2020][\"Price\"].min())\n",
    "y_min = [before_referendum_brexit_min, during_negotiations_brexit_min, after_brexit_min]\n",
    "X_min = [\"Before Referendum\", \"During Negotiations for Brexit\", \"Brexit Begins\"]\n",
    "\n",
    "print(\"Before Referendum -\", before_referendum_brexit_min)\n",
    "print(\"During Negotiations for Brexit -\", during_negotiations_brexit_min)\n",
    "print(\"Brexit Begins -\", after_brexit_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "vzWTVks50s5A",
    "outputId": "e5aa1adb-e752-488d-8f8f-a1c4dc1a79e7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.title(\"Analyzing Minimum Yearly Average House Prices in UK during Brexit Transition\")\n",
    "container_min = plt.bar(X_min, y_min, width = 0.5, color = colors, alpha = 0.5)\n",
    "plt.bar_label(container_min, labels = y_min, padding = 10)\n",
    "plt.xlabel(\"Brexit\", labelpad = 20)\n",
    "plt.ylabel(\"Minimum House Price\", labelpad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-z1w5-4008m"
   },
   "source": [
    "    --- CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIvHGtay05kl"
   },
   "source": [
    "Brexit was a turning point in the history of the United Kingdom. It marked the day the United Kingdom broke away from European Union. On the **23rd of June, 2016**, a referendum was held to decide the position of the United Kingdom in realtion to Brexit. The transition and withdrawal from the European Union was one that went on for years. On the **31st of January, 2020**, the brexit agreements were finalised and came into force.\n",
    "\n",
    "We attempt understanding the impact of Brexit on house prices in the United Kingdom. This\n",
    "relationship is analysed using 4 approaches:\n",
    "\n",
    "    - Average yearly house prices in the United Kingdom\n",
    "    - Average monthly house prices in the United Kingdom\n",
    "    - Average maximum yearly house prices in the United Kingdom\n",
    "    - Average minimum yearly house prices in the United Kingdom\n",
    "    \n",
    "The average yearly house prices in the UK gives an overall idea and glance into how house prices fluctuate in 3 periods, these are:\n",
    "\n",
    "    - Before Brexit\n",
    "    - During Negotiations and Transition\n",
    "    - After Brexit.\n",
    "    \n",
    "Across these periods we analysed, we notice a trend of prices generally peaking each year. Two significant points worth noticing were between the year 2016, the year negotiations for Brexit began. We see a spike in average house prices between then and 2017. This period recorded the highest shift in yearly average prices in the UK, going from **309,720** in **2016** to **346,982** in **2017**. The other period worth noticing is the peak yearly average house price in UK which occured in **2022**, the figure amounting to **393,173**.\n",
    "\n",
    "The average monthly house prices in the UK gives a clearer picture of the fluctuations in house prices before brexit, during negotiations, and after brexit. The key take away from this analysis is the stability in the prices across different years before brexit compared to during negotiations, and after brexit. We see slight increase in the average house prices in the years before brexit, however, this increase has little oscillation across different months. During negotiations and the transition towards brexit, we start seeing strong oscillation in the average monthly house prices. The stablilty which we could see in our graph before brexit dwindles. The period after brexit sees the yearly average house prices in the UK reach it's peak while witnessing stronger oscillation of the prices. This flunctuations witnessed could be due to uncertainty, interest rates, affected trade relations, and other factors that the United Kingdom has had to deal with after brexit.\n",
    "\n",
    "In our bar chart, we analyse the maximum and minimum yearly average house prices in the United Kingdom.\n",
    "Our analysis focuses on the 3 periods previously specified. We see the following results:\n",
    "\n",
    "    - Average maximum yearly house prices in the United Kingdom:\n",
    "        * Before Brexit - 309,720\n",
    "        * During Transition - 365,597\n",
    "        * After Brexit - 393,173\n",
    "\n",
    "    - Average minimum yearly house prices in the United Kingdom:\n",
    "        * Before Brexit - 231,631\n",
    "        * During Transition - 346,982\n",
    "        * After Brexit - 352,649\n",
    "From our analysis we see the periods before brexit have the minimum average yearly house prices in the United Kingdom, while the period after brexit has the maximum average yearly house pricesin the United Kingdom. The difference in the average maximum and minimum yearly house prices in the United Kingdom between the period before and after brexit illustrates the consisitent rise in house prices and the big impact brexit played in house price increase.\n",
    "\n",
    "**NOTE:** All figures are in **POUNDS** which is the currency associated with the United Kingdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v43hfmnN4dgW"
   },
   "source": [
    "**SOLUTION D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFDZb0W8DWYd"
   },
   "source": [
    "Dropping the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEj2ywXsDbMA"
   },
   "outputs": [],
   "source": [
    "# From the Feature X\n",
    "X = X.drop(\"TDate\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Lo4MYBrg26"
   },
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouAqEYIirjqb",
    "outputId": "054f9bed-d7e5-47f8-8474-5fa6a2497889"
   },
   "outputs": [],
   "source": [
    "# Feature selection - To select best features\n",
    "selector = SelectKBest(f_regression, k = 2)\n",
    "X = selector.fit_transform(X, y)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raFtCUXDsObU",
    "outputId": "3e6ecc46-c5f6-47e3-ec32-dabbcad3004b"
   },
   "outputs": [],
   "source": [
    "# Highlighting features score gotten from SelectKBest during Feature Selection process\n",
    "feature_importance = {feature: score for feature, score in zip(selector.feature_names_in_, selector.scores_)}\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "618QZFlXsm0V"
   },
   "source": [
    "Splitting data into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lv4dM32ssX0"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYWhRQpu6WMB"
   },
   "source": [
    "Scaling the features using StandardScaler. Technique employed is standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOUjUUFL0455"
   },
   "outputs": [],
   "source": [
    "# Scaling the X Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pafdLa_963eH"
   },
   "source": [
    "**Training at least 3 predictive models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgFic3r368wI"
   },
   "source": [
    "      --- Linear Regression\n",
    "      --- KNN Regressor\n",
    "      --- Gradient Boosting Regressor\n",
    "      --- Stochastic Gradient Descent Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57b_om_a-IO7"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNaBBzWn_Eer"
   },
   "source": [
    "This is the model training, prediction and evaluation phase. This will be done using the function created \"build_multiple_regressors\" that creates a dataframe and stores all information during the model building phase. Finally we save our prefered model then provide some conclusions on our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Z0Uk3609jc5"
   },
   "outputs": [],
   "source": [
    "regressors = [LinearRegression(),\n",
    "              KNeighborsRegressor(),\n",
    "              SGDRegressor(loss='squared_error',\n",
    "                            penalty='l2',\n",
    "                            alpha=0.0001,\n",
    "                            fit_intercept=True,\n",
    "                            max_iter=1000,\n",
    "                            tol=1e-3,\n",
    "                            learning_rate='optimal',\n",
    "                            early_stopping=False,\n",
    "                            validation_fraction=0.1,\n",
    "                            n_iter_no_change=5),\n",
    "              Ridge(alpha = 3.0),\n",
    "              Lasso(alpha = 3.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "7jEUfP9s9txM",
    "outputId": "2783f0a1-e1ed-4df8-b461-afed4d589d08"
   },
   "outputs": [],
   "source": [
    "algorithm_metrics, cross_validation_metrics, model_info = build_multiple_regressors(regressors, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(algorithm_metrics)\n",
    "print(\"\\n\\n\")\n",
    "print(cross_validation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOx8NckXEDA5"
   },
   "source": [
    "    --- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following algorithms, we create a predictive model for house prices in the United Kingdom.\n",
    "    - Linear Regression\n",
    "    - KNN Regression\n",
    "    - Stochastic Gradient Descent Regression\n",
    "    - Ridge Regression\n",
    "    - Lasso Regression\n",
    "We evaluate these algorithms with following metrics\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "    - R-Squared (Coefficient of Determination)\n",
    "    - Cross Validation Mean\n",
    "    - Cross Validation Standard Deviation\n",
    "    - Fit Time\n",
    "    - Score Time\n",
    "    \n",
    "From our model building phase, Linear, Lasso, and Ridge regression perform best in training, testing, and have \n",
    "the best cross validation mean score of 92.49. In our analysis, we note the huge variation in the predictions of\n",
    "the Stochastic Gradient Descent Regression as in it produces a test r-squared of -86.003 while having a cross validation\n",
    "mean of 89.84. Therefore, it is not a reliable model. The KNN Regressor has the lowest R-Squared and Cross Validation mean\n",
    "of 0.76 and 86.11 respectively. \n",
    "\n",
    "The Linear, Lasso, and Ridge Regression models have a Training R-Squared of 0.93 and a test R-Squared of 0.95 indcating \n",
    "how well it can generalize. When analysing their cross validation statistics, we see the best algorithm is the Lasso\n",
    "regression as it records the lowest fit time and score time indicating how fast it is, while recording the highest\n",
    "cross validation mean for test evaluation of 0.967182. This is slightly higher than the Ridge and Linear regression which\n",
    "had similar cross validation test score of 0.967181. Regardless, Lasso regression remains our best model because it remains\n",
    "the model with the best fit time and score time as well.\n",
    "\n",
    "The model with the worst fit to our data with a test cross validation score of -124.536 is the Stochastic Gradient Descent\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dejKbuh0EHe6"
   },
   "outputs": [],
   "source": [
    "# Saving the best model\n",
    "save_model = save_model_from_cross_validation(models_info = model_info, \n",
    "                                              algorithm = \"Lasso\", \n",
    "                                              index = 9)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
